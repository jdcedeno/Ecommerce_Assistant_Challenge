Story,Task,Estimate min (hr),Estimate max (hr),Actual,Comment
01 – research,learn enough to be able to plan a sprint. The outcome of this task should be a completed time estimate file.,none,none,5 days 12.26 – 12.30,Learning about prompt engineering and building LLM systems with langchain. Understanding vector stores and semanting space representations of text for running similarity search. Tinkering with python and langchain. Trying to build simple systems and understand the framework and workflows.
02 – Build Retriever,tinker with libraries and python code to build the simplest system that retrieves the relevant documents,,,4 days 01.02 – 01.05,
03 – Build LLM,Build the simplest system that answers questions based on retrieved relevant documents,,,1 day 01.06,
04 – Build Test site,Build a node-express server that can run a python program. Configure Docker,4h,8h,1h,"Build a simple system that implements the same workflow the app will use. The server will have an endpoint that receives the user query for the LLM. The LLM will be a python script that takes one string (user query) as the only argument.
10 min – update docker desktop
50 min – Build simple server with docker configuration."
04 – Build Test site,Build the simplest client and server apps so users can query the llm from a client application.,8h,12h,8h,"1h – create python script for retrieving documents from query using similarity search. Return the 3 most relevant documents
4h – Build python script for building and querying a chromadb collection. At this point the script has been modified to take a –reset flag that defaults to false. This assumes the collection already exists. Needs more flexibility when choosing which collection to query. (In progress).
1h20m – Test chroma similarity search results when embedding documents with or without metadatas and title. Found out that similarity is only performed on the actual stuff that was embedded. Conclusion: should embed the full row from product data.
20m – Start building tool picker logic. Should be a LLM that receives a query and decides which tool to use depending on the user query.
Try to finish building tool picker:
     1H20m – Building workflow for script to query chroma. Must be containerized so libraries will be pre-installed to prevent modulenotfound error. Run the subprocess.check_output() command to execute the container to query chromadb."
05 – Deploy,Configure Nginx on the host machine and set up docker containers and docker-compose file. Set up production environment. Test,,,,
01 – research,build a small system that can use few-shot examples to decide which tool to use based on a user query,4h,8h,,2h30m – Research how LLMs can use tools
06 – Build Tool-Picking LLM,Build a small LLM that can decide which tool to use based on user prompt. Tools should be implemented in a different task. The goal of this task is only to build an llm that can correcty decide which tool to use given their descriptions. No execution yet,8h,12h,6h,"6h – First step: build a basic LLM that can decide which tool to use given a user query. The LLM should output the tool to be used to get the required information based on the user prompt.
     - May need to have the LLM take the user query and structure the data (in the case of the mock API tool) for use as input"
07 – Build tool execution workflow,"Add logic that parses the result from the LLM, runs whichever tools are required and returns the results in some convenient format",1h,2h,45m,
08 – queryRobertaQA.py,"create a script that takes a user query, uses the toolPicker.py to get data for context, and uses the serverless inference API to query the Roberta base squad model for extractive QA",8h,12h,8h30m,Next steps: Integrate mockAPI and chromadb into tool usage. Tell took picker model how to use the mockAPI to get user information
01 – research,Learn more about agents and tool calling concepts,1h,3h,2h50m,"https://huggingface.co/docs/smolagents/index
https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents
https://huggingface.co/papers/2411.01747
https://huggingface.co/docs/smolagents/conceptual_guides/react
https://huggingface.co/blog/open-source-llms-as-agents
https://colab.research.google.com/drive/1j_vsc28FwZEDocDxVxWJ6Fvxd18FK8Gl?usp=sharing"
09 – Clean up PC for running DeepSeekR1 locally (there is no serverless inference api),"1. Make space in PC, probably purchase a new one that can run inference for LLMs. ROG Zephyrus G16 (2025) GU605 sounds good.
2. Enable LLMs to run using graphics card",1h,2h,7h,"- Delete old user profile, around 30GB
- Create Powershell program to analyze total size of each item in a given path or drive. Include nested items when calculating total size of container types like libraries
- Delete large unused files and folders
- Delete unused programs"
03 – Build LLM,Build and run queries on the DeepSeekR1 model. Make sure it runs inference on the GPU.,1h,5h,1h20m,"- Install dependencies to run model
- Verify CUDA version and install compatible PyTorch version"
03 – Build LLM,Build and run queries on the DeepSeekR1 model. Make sure to use serverless inference API.,2h,4h,1h40m,"- update huggingface_hub, was causing issues when running inference"
03 – Build LLM Prompt,"Create a prompt tailored for DeepSeekR1, the user message should contain all instructions. System role messages shouldn’t be used (from the docs)",4h,8h,2h30m,"- test out different prompt ideas (1h)
- add tool calling logic after getting tool suggestions (1h30m)
- setup server using uvicorn. (10m)"
10 – Build tool-calling logic,"Parse the tool-suggestion response, call the tools and craft a new prompt for answering the user’s query given the new context obtained from calling the suggested tools",4h,8h,6h20m,"1. start (10am): analyze current codebase and start organizing into services, start managing individual environments. [duration: 30m, time: 10:35am]
2. start (6am): 
     - Test FastAPI projects (mockAPI and query_llm_service). [duration: 15m, time: 6:15am]
     - Add missing tools in prompt, fix tool mapping in execution logic. [duration: 1h, time: 7:15am]
3. start (8:10am):
     - Debug tool execution logic. Make sure response is parsed correctly and tools are executed. [duration: 1h10m, time: 9:20am]
4. start (9:50am):
     - Test different endpoints (shipping-summary has a bug, not in the scope to fix it). Fix tool-calling logic when no arguments given. [duration: 20m, time: 10:10am]
     - Craft new prompt for answering query based on context. Handle case when context is too large. [start: 10:20am, duration: 1h25m, time: 11:45am]
     - Build express server. [start: 4:10pm, duration: 1h40m, time: 5:50pm]"
11 – Build client,"Build a simple client with:
1. an input box for the user to type a query.
2. a button to submit the user query:
- the button should submit a post request with the user query to the server’s /query endpoint.
- the response contains llm_response, tool_results, and answer. These should be showed in a readable format. Show initial thoughts for tool suggestion (llm_response), suggested tools (tool_results), and the final answer to the query (answer) in a readable way.
- apply the minimum styles so it looks presentable.",4h,8h,,"1. start (6am):
     - analyze current codebase, re-run tests (from client to server to services consuming the mockAPI). [duration: 1h, time: 7am]
     - build vite + react client. Remove unused files. Configure eslint and prettier. [start: 10am, duration: 3h, time: 1:00pm]"
